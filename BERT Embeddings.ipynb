{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66448de3-fc27-4779-8240-9754ad92f7ad",
   "metadata": {},
   "source": [
    "# BERT Embeddings\n",
    "## Featuring: DistilBERT on PyTorch\n",
    "\n",
    "This is adapted from HuggingFace example codes: https://huggingface.co/docs/transformers/model_doc/distilbert\n",
    "\n",
    "### Preliminaries: Load the text tokenizer and deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba433e-1a34-4839-9301-652f468dedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f34681c-4099-4d48-8b73-10414db197f5",
   "metadata": {},
   "source": [
    "### Create some input text and convert to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b5b2ff-1909-458c-87ed-9affd3c40a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Hey, my dog is cute\",\n",
    "             \"Hey, my dog is ugly\",\n",
    "             \"Hey, my dog is not cute\",\n",
    "             \"I put my money in the bank\",\n",
    "             \"Banks have lots of money\",\n",
    "             \"Hey, my dog is by river bank\"]\n",
    "inputs = tokenizer(sentences,\n",
    "                   padding=True,\n",
    "                   return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b8a2a-2c8a-4e46-aba3-e1e1b3ba1b62",
   "metadata": {},
   "source": [
    "### Process the inputs and obtain the CLS token embeddings (whole sentence meaning)\n",
    "\n",
    "Note that the text information for each sentence is now represented as a 768-dimensional vector. Similar vectors (close according to distance and/or dot-product computations) should be found for text with similar meanings. However, dissimilar vectors should correspond to dissimilar texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87f0e7a-0c45-45cc-ae09-409a8860934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(**inputs).last_hidden_state[:,0,:]\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184aa55b-30b4-4f75-abc3-730af3957a47",
   "metadata": {},
   "source": [
    "### Quick, unscaled dot-product comparison between all embeddings\n",
    "\n",
    "Note that higher values indicate more similarity when using dot-product. We could also calculate distances, which might be more appropriate in some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcea851-a950-4385-806a-d9310c17cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "output @ output.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797068d-f995-4c03-8c9f-00e561890793",
   "metadata": {},
   "source": [
    "### Euclidean distances between all embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec2fb4-3999-4b39-a8c2-d2b3bbb7ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "squareform(pdist(output.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb925296-3c60-4633-ab34-6ac7bb3054a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "import matplotlib.pyplot as plt\n",
    "offset = 0.1\n",
    "mds = MDS(n_components=2,dissimilarity='euclidean')\n",
    "projection = mds.fit_transform(output.numpy())\n",
    "plt.scatter(projection[:,0],projection[:,1])\n",
    "for i,x in enumerate(projection):\n",
    "    plt.text(x[0]+offset,x[1],'%d - %s'%(i,sentences[i]))\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f1bc78-18d5-4ede-8b3d-ccc515f81127",
   "metadata": {},
   "source": [
    "### Information distances between all embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a22f51-7dca-4eb7-a077-f13e5b49ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = -1.0 * torch.log(((torch.softmax(output @ output.T,0) *\n",
    "                            torch.softmax(output @ output.T,1))))\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576ac154-d802-43ab-9c08-60dfb19ed866",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ed6b1-e5a2-404e-8d35-a4f1726d6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b954b-d750-4bc9-9fe1-3f9e4f7d17b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "import matplotlib.pyplot as plt\n",
    "offset = 0.1\n",
    "mds = MDS(n_components=2,dissimilarity='precomputed')\n",
    "projection = mds.fit_transform(logits.numpy())\n",
    "plt.scatter(projection[:,0],projection[:,1])\n",
    "for i,x in enumerate(projection):\n",
    "    plt.text(x[0]+offset,x[1],'%d - %s'%(i,sentences[i]))\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d93ad98-ff6b-41f9-993d-802109623425",
   "metadata": {},
   "source": [
    "### Obtain some summary/visualization info on the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f501c5e7-9c74-4f71-bd96-9937769a1725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "from torchview import draw_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e0ab9b-a1a0-4a6e-b39d-4f7447da542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eb32db-4be7-4c95-b1bb-e6123b05abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graph = draw_graph(model,input_data=inputs)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496404f-230b-44dc-8921-8ff770f6e9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
